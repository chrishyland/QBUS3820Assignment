{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#Stop warnings\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ATM_Training.csv')\n",
    "y = data.pop('Withdraw')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    \"\"\"Baseline neural network model by playing around with parameters.\"\"\"\n",
    "    #Construct a neural network object:\n",
    "    model = Sequential()\n",
    "    #Input dimension for input layer MUST match the number of predictors in our dataset. Output dimension tells us \n",
    "    #how many edges going into next layer (can be any number from 1 to number of vertices in next layer). \n",
    "    #Kernel_init tells us what initial weightings to set the weights when training it. Relu is activation function:\n",
    "    model.add(Dense(input_dim=6, output_dim = 60, kernel_initializer='normal', activation='relu'))\n",
    "    #Here we created a layer with 60 vertices and the same parameters as before.\n",
    "    model.add(Dense(60, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(60, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(15, kernel_initializer='normal', activation = 'relu'))    \n",
    "    model.add(Dense(6, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation = 'relu'))\n",
    "    # Compile model from this. Keep this loss function whilst ADAM is a better optimizer than SGD\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "#0.424704037722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Need to transform from pandas df into matrix for keras to work with. Then since alot of the predictors are dummy\n",
    "#variables, this means that when training and learning, the model won't update since alot of 0's in our data.\n",
    "#Therefore, add a small number to ensure we update the parameters.\n",
    "xarray = X_train.as_matrix()\n",
    "xarray = xarray+0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "14740/14740 [==============================] - 1s - loss: 941.7071     \n",
      "Epoch 2/25\n",
      "14740/14740 [==============================] - 1s - loss: 17.7536     \n",
      "Epoch 3/25\n",
      "14740/14740 [==============================] - 2s - loss: 8.0950     \n",
      "Epoch 4/25\n",
      "14740/14740 [==============================] - 1s - loss: 7.3200     \n",
      "Epoch 5/25\n",
      "14740/14740 [==============================] - 2s - loss: 7.3550     \n",
      "Epoch 6/25\n",
      "14740/14740 [==============================] - 1s - loss: 7.0981     \n",
      "Epoch 7/25\n",
      "14740/14740 [==============================] - 2s - loss: 7.3191     \n",
      "Epoch 8/25\n",
      "14740/14740 [==============================] - 2s - loss: 7.0927     \n",
      "Epoch 9/25\n",
      "14740/14740 [==============================] - 1s - loss: 7.1400     \n",
      "Epoch 10/25\n",
      "14740/14740 [==============================] - 2s - loss: 6.9638     \n",
      "Epoch 11/25\n",
      "14740/14740 [==============================] - 1s - loss: 6.8855     \n",
      "Epoch 12/25\n",
      "14740/14740 [==============================] - 2s - loss: 6.4682     \n",
      "Epoch 13/25\n",
      "14740/14740 [==============================] - 2s - loss: 6.3078     \n",
      "Epoch 14/25\n",
      "14740/14740 [==============================] - 1s - loss: 5.6644     \n",
      "Epoch 15/25\n",
      "14740/14740 [==============================] - 2s - loss: 4.8045     \n",
      "Epoch 16/25\n",
      "14740/14740 [==============================] - 1s - loss: 4.0898     \n",
      "Epoch 17/25\n",
      "14740/14740 [==============================] - 2s - loss: 3.5026     \n",
      "Epoch 18/25\n",
      "14740/14740 [==============================] - 1s - loss: 2.8520     \n",
      "Epoch 19/25\n",
      "14740/14740 [==============================] - 1s - loss: 2.3376     \n",
      "Epoch 20/25\n",
      "14740/14740 [==============================] - 2s - loss: 1.9952     \n",
      "Epoch 21/25\n",
      "14740/14740 [==============================] - 1s - loss: 1.7290     \n",
      "Epoch 22/25\n",
      "14740/14740 [==============================] - 2s - loss: 1.4671     \n",
      "Epoch 23/25\n",
      "14740/14740 [==============================] - 1s - loss: 1.3082     \n",
      "Epoch 24/25\n",
      "14740/14740 [==============================] - 2s - loss: 1.1894     \n",
      "Epoch 25/25\n",
      "14740/14740 [==============================] - 1s - loss: 1.0309     \n",
      "Final MSE is  0.928453000594\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(xarray, y_train, nb_epoch=25, batch_size=50)\n",
    "predictions = model.predict(X_test.as_matrix())\n",
    "test = list(predictions)\n",
    "print('Final MSE is ',mean_squared_error(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model I got was the best one I've found so from just playing around with the parameters. However, it is from far being an optimal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
